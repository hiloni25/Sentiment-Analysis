{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to make sure everything has been loaded in correctly, we can look at the dimensions of the vocabulary list and the embedding matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also search our word list for a word like \"baseball\", and then access its corresponding vector through the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9327  ,  1.0421  , -0.78515 ,  0.91033 ,  0.22711 , -0.62158 ,\n",
       "       -1.6493  ,  0.07686 , -0.5868  ,  0.058831,  0.35628 ,  0.68916 ,\n",
       "       -0.50598 ,  0.70473 ,  1.2664  , -0.40031 , -0.020687,  0.80863 ,\n",
       "       -0.90566 , -0.074054, -0.87675 , -0.6291  , -0.12685 ,  0.11524 ,\n",
       "       -0.55685 , -1.6826  , -0.26291 ,  0.22632 ,  0.713   , -1.0828  ,\n",
       "        2.1231  ,  0.49869 ,  0.066711, -0.48226 , -0.17897 ,  0.47699 ,\n",
       "        0.16384 ,  0.16537 , -0.11506 , -0.15962 , -0.94926 , -0.42833 ,\n",
       "       -0.59457 ,  1.3566  , -0.27506 ,  0.19918 , -0.36008 ,  0.55667 ,\n",
       "       -0.70315 ,  0.17157 ], dtype=float32)"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('baseball')\n",
    "wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have our vectors, our first step is taking an input sentence and then constructing the its vector representation. Let's say that we have the input sentence \"I thought the movie was incredible and inspiring\". In order to get the word vectors, we can use Tensorflow's embedding lookup function. This function takes in two arguments, one for the embedding matrix (the wordVectors matrix in our case), and one for the ids of each of the words. The ids vector can be thought of as the integerized representation of the training set. This is basically just the row index of each of the words. Let's look at a quick example to make this concrete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 10 x 50 output should contain the 50 dimensional word vectors for each of the 10 words in the sequence.\n",
    "#Lets display the word vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing the input files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "('The total number of files is', 25000)\n",
      "('The total number of words in the files is', 5844464)\n",
      "('The average number of words in the files is', 233)\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\") as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\") as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The training set we're going to use is the Imdb movie review dataset. This set has 25,000 movie reviews, with 12,500 positive reviews and 12,500 negative reviews. Each of the reviews is stored in a txt file that we need to parse through. The positive reviews are stored in one directory and the negative reviews are stored in another. The following piece of code will determine total and average number of words in each review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also use the Matplot library to visualize this data in a histogram format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the histogram as well as the average number of words per file, we can safely say that most reviews will fall under 250 words, which is the max sequence length value we will set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how we can take a single file and transform it into our ids matrix. This is what one of the reviews looks like in text file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a strange feeling to sit alone in a theater occupied by parents and their rollicking kids. I felt like instead of a movie ticket, I should have been given a NAMBLA membership.<br /><br />Based upon Thomas Rockwell's respected Book, How To Eat Fried Worms starts like any children's story: moving to a new town. The new kid, fifth grader Billy Forrester was once popular, but has to start anew. Making friends is never easy, especially when the only prospect is Poindexter Adam. Or Erica, who at 4 1/2 feet, is a giant.<br /><br />Further complicating things is Joe the bully. His freckled face and sleeveless shirts are daunting. He antagonizes kids with the Death Ring: a Crackerjack ring that is rumored to kill you if you're punched with it. But not immediately. No, the death ring unleashes a poison that kills you in the eight grade.<br /><br />Joe and his axis of evil welcome Billy by smuggling a handful of slimy worms into his thermos. Once discovered, Billy plays it cool, swearing that he eats worms all the time. Then he throws them at Joe's face. Ewww! To win them over, Billy reluctantly bets that he can eat 10 worms. Fried, boiled, marinated in hot sauce, squashed and spread on a peanut butter sandwich. Each meal is dubbed an exotic name like the \"Radioactive Slime Delight,\" in which the kids finally live out their dream of microwaving a living organism.<br /><br />If you've ever met me, you'll know that I have an uncontrollably hearty laugh. I felt like a creep erupting at a toddler whining that his \"dilly dick\" hurts. But Fried Worms is wonderfully disgusting. Like a G-rated Farrelly brothers film, it is both vomitous and delightful.<br /><br />Writer/director Bob Dolman is also a savvy storyteller. To raise the stakes the worms must be consumed by 7 pm. In addition Billy holds a dark secret: he has an ultra-sensitive stomach.<br /><br />Dolman also has a keen sense of perspective. With such accuracy, he draws on children's insecurities and tendency to exaggerate mundane dilemmas.<br /><br />If you were to hyperbolize this movie the way kids do their quandaries, you will see that it is essentially about war. Freedom-fighter and freedom-hater use pubescent boys as pawns in proxy wars, only to learn a valuable lesson in unity. International leaders can learn a thing or two about global peacekeeping from Fried Worms.<br /><br />At the end of the film, I was comforted when two chaperoning mothers behind me, looked at each other with befuddlement and agreed, \"That was a great movie.\" Great, now I won't have to register myself in any lawful databases.\n"
     ]
    }
   ],
   "source": [
    "fname = positiveFiles[3] #Can use any valid index (not just 3)\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a strange feeling to sit alone in a theater occupied by parents and their rollicking kids. I felt like instead of a movie ticket, I should have been given a NAMBLA membership.<br /><br />Based upon Thomas Rockwell's respected Book, How To Eat Fried Worms starts like any children's story: moving to a new town. The new kid, fifth grader Billy Forrester was once popular, but has to start anew. Making friends is never easy, especially when the only prospect is Poindexter Adam. Or Erica, who at 4 1/2 feet, is a giant.<br /><br />Further complicating things is Joe the bully. His freckled face and sleeveless shirts are daunting. He antagonizes kids with the Death Ring: a Crackerjack ring that is rumored to kill you if you're punched with it. But not immediately. No, the death ring unleashes a poison that kills you in the eight grade.<br /><br />Joe and his axis of evil welcome Billy by smuggling a handful of slimy worms into his thermos. Once discovered, Billy plays it cool, swearing that he eats worms all the time. Then he throws them at Joe's face. Ewww! To win them over, Billy reluctantly bets that he can eat 10 worms. Fried, boiled, marinated in hot sauce, squashed and spread on a peanut butter sandwich. Each meal is dubbed an exotic name like the \"Radioactive Slime Delight,\" in which the kids finally live out their dream of microwaving a living organism.<br /><br />If you've ever met me, you'll know that I have an uncontrollably hearty laugh. I felt like a creep erupting at a toddler whining that his \"dilly dick\" hurts. But Fried Worms is wonderfully disgusting. Like a G-rated Farrelly brothers film, it is both vomitous and delightful.<br /><br />Writer/director Bob Dolman is also a savvy storyteller. To raise the stakes the worms must be consumed by 7 pm. In addition Billy holds a dark secret: he has an ultra-sensitive stomach.<br /><br />Dolman also has a keen sense of perspective. With such accuracy, he draws on children's insecurities and tendency to exaggerate mundane dilemmas.<br /><br />If you were to hyperbolize this movie the way kids do their quandaries, you will see that it is essentially about war. Freedom-fighter and freedom-hater use pubescent boys as pawns in proxy wars, only to learn a valuable lesson in unity. International leaders can learn a thing or two about global peacekeeping from Fried Worms.<br /><br />At the end of the film, I was comforted when two chaperoning mothers behind me, looked at each other with befuddlement and agreed, \"That was a great movie.\" Great, now I won't have to register myself in any lawful databases.\n"
     ]
    }
   ],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for summarizing the tweet \n",
    "from collections import Counter\n",
    "from math import fabs\n",
    "from re import split as regex_split, sub as regex_sub, UNICODE as REGEX_UNICODE\n",
    "\n",
    "#defining stop words\n",
    "stopWords = set([\n",
    "    \"-\", \" \", \",\", \".\", \"a\", \"e\", \"i\", \"o\", \"u\", \"t\", \"about\", \"above\",\n",
    "    \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\",\n",
    "    \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\",\n",
    "    \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\",\n",
    "    \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\",\n",
    "    \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\",\n",
    "    \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\",\n",
    "    \"between\", \"beyond\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\",\n",
    "    \"cannot\", \"can't\", \"co\", \"con\", \"could\", \"couldn't\", \"de\",\n",
    "    \"describe\", \"detail\", \"did\", \"do\", \"done\", \"down\", \"due\", \"during\",\n",
    "    \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\",\n",
    "    \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\",\n",
    "    \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\",\n",
    "    \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\",\n",
    "    \"further\", \"get\", \"give\", \"go\", \"got\", \"had\", \"has\", \"hasnt\",\n",
    "    \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\",\n",
    "    \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"into\", \"is\", \"it\", \"its\", \"it's\", \"itself\", \"just\", \"keep\", \"last\",\n",
    "    \"latter\", \"latterly\", \"least\", \"less\", \"like\", \"ltd\", \"made\", \"make\",\n",
    "    \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\",\n",
    "    \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\",\n",
    "    \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\",\n",
    "    \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\",\n",
    "    \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\",\n",
    "    \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "    \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"people\", \"per\",\n",
    "    \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"said\", \"same\", \"see\",\n",
    "    \"seem\", \"seemed\", \"seeming\", \"seems\", \"several\", \"she\", \"should\",\n",
    "    \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "    \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\",\n",
    "    \"somewhere\", \"still\", \"such\", \"take\", \"ten\", \"than\", \"that\", \"the\",\n",
    "    \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\",\n",
    "    \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\",\n",
    "    \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\",\n",
    "    \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\",\n",
    "    \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\",\n",
    "    \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"use\", \"very\",\n",
    "    \"via\", \"want\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\",\n",
    "    \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\",\n",
    "    \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\",\n",
    "    \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\",\n",
    "    \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\",\n",
    "    \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"reuters\", \"news\",\n",
    "    \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\",\n",
    "    \"sunday\", \"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\",\n",
    "    \"rappler\", \"rapplercom\", \"inquirer\", \"yahoo\", \"home\", \"sports\",\n",
    "    \"1\", \"10\", \"2012\", \"sa\", \"says\", \"tweet\", \"pm\", \"home\", \"homepage\",\n",
    "    \"sports\", \"section\", \"newsinfo\", \"stories\", \"story\", \"photo\",\n",
    "    \"2013\", \"na\", \"ng\", \"ang\", \"year\", \"years\", \"percent\", \"ko\", \"ako\",\n",
    "    \"yung\", \"yun\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"time\",\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\",\n",
    "    \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "    \"government\", \"police\"\n",
    "])\n",
    "ideal = 20.0\n",
    "\n",
    "\n",
    "#def SummarizeUrl(url):\n",
    " #   summaries = []\n",
    "  #  try:\n",
    "   #     article = grab_link(url)\n",
    "    #except IOError:\n",
    "     #   print 'IOError'\n",
    "      #  return None\n",
    "\n",
    "    #if not (article and article.cleaned_text and article.title):\n",
    "     #   return None\n",
    "\n",
    "    #summaries = Summarize(unicode(article.title),\n",
    "     #                     unicode(article.cleaned_text))\n",
    "    #return summaries\n",
    "\n",
    "\n",
    "def Summarize(text):\n",
    "    summaries = []\n",
    "    sentences = split_sentences(text)\n",
    "    keys = keywords(text)\n",
    "    #titleWords = split_words(title)\n",
    "\n",
    "    if len(sentences) <= 5:\n",
    "        return sentences\n",
    "\n",
    "    #score setences, and use the top 5 sentences\n",
    "    ranks = score(sentences, keys).most_common(5)\n",
    "    for rank in ranks:\n",
    "        summaries.append(rank[0])\n",
    "        print(\"yuss\")\n",
    "        \n",
    "    print(summaries)\n",
    "    return summaries\n",
    "\n",
    "\n",
    "#def grab_link(inurl):\n",
    "    #extract article information using Python Goose\n",
    " #   from goose import Goose\n",
    "  #  try:\n",
    "   #     article = Goose().extract(url=inurl)\n",
    "    #    return article\n",
    "    #except ValueError:\n",
    "     #   print 'Goose failed to extract article from url'\n",
    "      #  return None\n",
    "    #return None\n",
    "\n",
    "\n",
    "def score(sentences, keywords):\n",
    "    #score sentences based on different features\n",
    "\n",
    "    senSize = len(sentences)\n",
    "    ranks = Counter()\n",
    "    for i, s in enumerate(sentences):\n",
    "        sentence = split_words(s)\n",
    "        #titleFeature = title_score(titleWords, sentence)\n",
    "        sentenceLength = length_score(sentence)\n",
    "        sentencePosition = sentence_position(i+1, senSize)\n",
    "        sbsFeature = sbs(sentence, keywords)\n",
    "        dbsFeature = dbs(sentence, keywords)\n",
    "        frequency = (sbsFeature + dbsFeature) / 2.0 * 10.0\n",
    "\n",
    "        #weighted average of scores from four categories\n",
    "        totalScore = (frequency*2.0 + sentenceLength*1.0 + sentencePosition*1.0) / 4.0\n",
    "        ranks[s] = totalScore\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def sbs(words, keywords):\n",
    "    score = 0.0\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    for word in words:\n",
    "        if word in keywords:\n",
    "            score += keywords[word]\n",
    "    return (1.0 / fabs(len(words)) * score)/10.0\n",
    "\n",
    "\n",
    "def dbs(words, keywords):\n",
    "    if (len(words) == 0):\n",
    "        return 0\n",
    "\n",
    "    summ = 0\n",
    "    first = []\n",
    "    second = []\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in keywords:\n",
    "            score = keywords[word]\n",
    "            if first == []:\n",
    "                first = [i, score]\n",
    "            else:\n",
    "                second = first\n",
    "                first = [i, score]\n",
    "                dif = first[0] - second[0]\n",
    "                summ += (first[1]*second[1]) / (dif ** 2)\n",
    "\n",
    "    # number of intersections\n",
    "    k = len(set(keywords.keys()).intersection(set(words))) + 1\n",
    "    return (1/(k*(k+1.0))*summ)\n",
    "\n",
    "\n",
    "def split_words(text):\n",
    "    #split a string into array of words\n",
    "    try:\n",
    "        text = regex_sub(r'[^\\w ]', '', text, flags=REGEX_UNICODE)  # strip special chars\n",
    "        return [x.strip('.').lower() for x in text.split()]\n",
    "    except TypeError:\n",
    "        print \"Error while splitting characters\"\n",
    "        return None\n",
    "\n",
    "\n",
    "def keywords(text):\n",
    "    #get the top 10 keywords and their frequency scores\n",
    "    #ignores blacklisted words in stopWords,\n",
    "    #counts the number of occurrences of each word\n",
    "\n",
    "    text = split_words(text)\n",
    "    numWords = len(text)  # of words before removing blacklist words\n",
    "    freq = Counter(x for x in text if x not in stopWords)\n",
    "\n",
    "    minSize = min(10, len(freq))  # get first 10\n",
    "    keywords = {x: y for x, y in freq.most_common(minSize)}  # recreate a dict\n",
    "\n",
    "    for k in keywords:\n",
    "        articleScore = keywords[k]*1.0 / numWords\n",
    "        keywords[k] = articleScore * 1.5 + 1\n",
    "\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    \n",
    "    #The regular expression matches all sentence ending punctuation and splits the string at those points.\n",
    "    #At this point in the code, the list looks like this [\"Hello, world\", \"!\" ... ]. The punctuation and all quotation marks\n",
    "    #are separated from the actual text. The first s_iter line turns each group of two items in the list into a tuple,\n",
    "    #excluding the last item in the list (the last item in the list does not need to have this performed on it). Then,\n",
    "    #the second s_iter line combines each tuple in the list into a single item and removes any whitespace at the beginning\n",
    "    #of the line. Now, the s_iter list is formatted correctly but it is missing the last item of the sentences list. The\n",
    "    #second to last line adds this item to the s_iter list and the last line returns the full list.\n",
    "    \n",
    "    sentences = regex_split(u'(?<![A-ZА-ЯЁ])([.!?]\"?)(?=\\s+\\\"?[A-ZА-ЯЁ])', text, flags=REGEX_UNICODE)\n",
    "    s_iter = zip(*[iter(sentences[:-1])] * 2)\n",
    "    s_iter = [''.join(map(unicode,y)).lstrip() for y in s_iter]\n",
    "    s_iter.append(sentences[-1])\n",
    "    return s_iter\n",
    "\n",
    "\n",
    "\n",
    "def length_score(sentence):\n",
    "    return 1 - fabs(ideal - len(sentence)) / ideal\n",
    "\n",
    "\n",
    "#def title_score(title, sentence):\n",
    " #   title = [x for x in title if x not in stopWords]\n",
    "  #  count = 0.0\n",
    "   # for word in sentence:\n",
    "    #    if (word not in stopWords and word in title):\n",
    "     #       count += 1.0\n",
    "            \n",
    "    #if len(title) == 0:\n",
    "     #   return 0.0\n",
    "        \n",
    "    #return count/len(title)\n",
    "\n",
    "\n",
    "def sentence_position(i, size):\n",
    "    #different sentence positions indicate different\n",
    "    #probability of being an important sentence\"\"\"\n",
    "\n",
    "    normalized = i*1.0 / size\n",
    "    if 0 < normalized <= 0.1:\n",
    "        return 0.17\n",
    "    elif 0.1 < normalized <= 0.2:\n",
    "        return 0.23\n",
    "    elif 0.2 < normalized <= 0.3:\n",
    "        return 0.14\n",
    "    elif 0.3 < normalized <= 0.4:\n",
    "        return 0.08\n",
    "    elif 0.4 < normalized <= 0.5:\n",
    "        return 0.05\n",
    "    elif 0.5 < normalized <= 0.6:\n",
    "        return 0.04\n",
    "    elif 0.6 < normalized <= 0.7:\n",
    "        return 0.06\n",
    "    elif 0.7 < normalized <= 0.8:\n",
    "        return 0.04\n",
    "    elif 0.8 < normalized <= 0.9:\n",
    "        return 0.04\n",
    "    elif 0.9 < normalized <= 1.0:\n",
    "        return 0.15\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yuss\n",
      "yuss\n",
      "yuss\n",
      "yuss\n",
      "yuss\n",
      "[u\"He antagonizes kids with the Death Ring: a Crackerjack ring that is rumored to kill you if you're punched with it.\", u'In addition Billy holds a dark secret: he has an ultra-sensitive stomach.<br /><br />Dolman also has a keen sense of perspective.', u'But Fried Worms is wonderfully disgusting.', u'Like a G-rated Farrelly brothers film, it is both vomitous and delightful.<br /><br />Writer/director Bob Dolman is also a savvy storyteller.', u'Freedom-fighter and freedom-hater use pubescent boys as pawns in proxy wars, only to learn a valuable lesson in unity.']\n"
     ]
    }
   ],
   "source": [
    "sumline = Summarize(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"He antagonizes kids with the Death Ring: a Crackerjack ring that is rumored to kill you if you're punched with it.\", u'In addition Billy holds a dark secret: he has an ultra-sensitive stomach.<br /><br />Dolman also has a keen sense of perspective.', u'But Fried Worms is wonderfully disgusting.', u'Like a G-rated Farrelly brothers film, it is both vomitous and delightful.<br /><br />Writer/director Bob Dolman is also a savvy storyteller.', u'Freedom-fighter and freedom-hater use pubescent boys as pawns in proxy wars, only to learn a valuable lesson in unity.']\n"
     ]
    }
   ],
   "source": [
    "print(sumline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = str(sumline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's convert to to an ids matrix\n",
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\n",
    "\n",
    "#def SummarizeUrl(url): instead of url we will use lines-hiloni\n",
    " #   summaries = []\n",
    "  #  try:\n",
    "   #     article = grab_link(url)\n",
    "    #except IOError:\n",
    "     #   print 'IOError'\n",
    "      #  return None\n",
    "\n",
    "    #if not (article and article.cleaned_text and article.title):\n",
    "     #   return None\n",
    "\n",
    "    #summaries = Summarize(unicode(article.title),unicode(article.cleaned_text))\n",
    "    #return summaries\n",
    "\n",
    "\n",
    "#def Summarize(title, text):\n",
    "\n",
    "\n",
    "#def Summarize(lines):\n",
    "\n",
    " #   summaries = []\n",
    "  #  sentences = split_sentences(lines)\n",
    "   # keys = keywords(text)\n",
    "    #titleWords = split_words(title)\n",
    "\n",
    "    #if len(sentences) <= 5:\n",
    "     #   return sentences\n",
    "\n",
    "    #score setences, and use the top 5 sentences\n",
    "    #ranks = score(sentences, titleWords, keys).most_common(5)\n",
    "    #ranks = score(sentences, keys).most_common(5)\n",
    "\n",
    "\n",
    "    #for rank in ranks:\n",
    "     #   summaries.append(rank[0])\n",
    "      #  print summaries\n",
    "\n",
    "    #return summaries\n",
    "\n",
    "\n",
    "#def grab_link(inurl):\n",
    "    #extract article information using Python Goose\n",
    " #   from goose import Goose\n",
    "  #  try:\n",
    "   #     article = Goose().extract(url=inurl)\n",
    "    #    return article\n",
    "   # except ValueError:\n",
    "    #    print 'Goose failed to extract article from url'\n",
    "     #   return None\n",
    "    #return None\n",
    "\n",
    "\n",
    "    #def score(sentences, titleWords, keywords):\n",
    "    #score sentences based on different features\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #def score(sentences, keywords):\n",
    "\n",
    "    #senSize = len(sentences)\n",
    "    #ranks = Counter()\n",
    "    #for i, s in enumerate(sentences):\n",
    "     #   sentence = split_words(s)\n",
    "        #titleFeature = title_score(titleWords, sentence)\n",
    "      #  sentenceLength = length_score(sentence)\n",
    "       # sentencePosition = sentence_position(i+1, senSize)\n",
    "        #sbsFeature = sbs(sentence, keywords)\n",
    "        #dbsFeature = dbs(sentence, keywords)\n",
    "        #frequency = (sbsFeature + dbsFeature) / 2.0 * 10.0\n",
    "\n",
    "        #weighted average of scores from four categories\n",
    "        #totalScore = (frequency*2.0 + sentenceLength*1.0 + sentencePosition*1.0) / 4.0\n",
    "        #ranks[s] = totalScore\n",
    "   # return ranks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "#using this in one block below below\n",
    "#import re\n",
    "#strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "#def cleanSentences(string):\n",
    "    #string = string.lower().replace(\"<br />\", \" \")\n",
    "    #xyz = [re.sub(strip_special_chars, \"\", string.lower()) for word in string]\n",
    "    #return(xyz)\n",
    "    #texts = [[word.lower() for word in text.split()] for text in data]\n",
    "    #return [re.sub(strip_special_chars, \"\", string.lower() for word in string]\n",
    "    #return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Back-up if i want to make new files(but not optimal solution)\n",
    "\n",
    "#firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "#with open(fname) as f:\n",
    " #   indexCounter = 0\n",
    "    \n",
    "  #  line=f.readline()\n",
    "   # flinesum=Summarize(line)\n",
    "    \n",
    "   # new_file=open(\"\",mode=\"a+\",encoding=\"utf-8\")\n",
    "   # new_file.writelines(flinesum)\n",
    "    \n",
    "   # for lines in new_file:\n",
    "    #    print(line)\n",
    "    #new_file.close()\n",
    "    \n",
    "    #cleanedLine = cleanSentences(lines)\n",
    "\n",
    "    #split = cleanedLine.split()\n",
    "    \n",
    "    #for word in split:\n",
    "            \n",
    "     #   if indexCounter < maxSeqLength:\n",
    "      #      try:\n",
    "       #         firstFile[indexCounter] = wordsList.index(word)\n",
    "        #    except ValueError:\n",
    "         #       firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        #indexCounter = indexCounter + 1\n",
    "#firstFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yuss\n",
      "yuss\n",
      "yuss\n",
      "yuss\n",
      "yuss\n",
      "[u\"He antagonizes kids with the Death Ring: a Crackerjack ring that is rumored to kill you if you're punched with it.\", u'In addition Billy holds a dark secret: he has an ultra-sensitive stomach.<br /><br />Dolman also has a keen sense of perspective.', u'But Fried Worms is wonderfully disgusting.', u'Like a G-rated Farrelly brothers film, it is both vomitous and delightful.<br /><br />Writer/director Bob Dolman is also a savvy storyteller.', u'Freedom-fighter and freedom-hater use pubescent boys as pawns in proxy wars, only to learn a valuable lesson in unity.']\n",
      "88\n",
      "cleaning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([205918, 167360,   1813,     17, 201534,    336,   2930,      7,\n",
       "       110855,   2930,     12,     14,  16215,      4,   1916,     81,\n",
       "           83, 211666,  14019,     17,     20, 354045,   1072,   4785,\n",
       "         2040,      7,   2237,   1779,     18,     31,     29, 232092,\n",
       "         7463, 120172,     52,     31,      7,   6281,   1380,      3,\n",
       "         5251, 399999,  10503,  16632,     14,  23275,  23967, 399999,\n",
       "            7,  23595,  58149,   1955,    319,     20,     14,    150,\n",
       "       399999,      5,  21593, 399999,   1862, 120172,     14,     52,\n",
       "            7,  12341,  32209, 399999,      5, 399999,    234,  73164,\n",
       "         2122,     19,  35793,      6,  12926,   3129,     91,      4,\n",
       "         2368,      7,   4640,   6557,      6,   3320,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0], dtype=int32)"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    \n",
    "    line=f.read()\n",
    "    flinesum=Summarize(line)\n",
    "    xyz = str(flinesum)\n",
    "    tokens = xyz.split()\n",
    "    n_tokens = len(tokens)\n",
    "    print (n_tokens)\n",
    "\n",
    "    #maxlen = len(xyz)\n",
    "    #print(maxlen)\n",
    "    #print(maxSeqLength)\n",
    "    #print(flinesum[0])\n",
    "    #xyz = flinesum[0]\n",
    "\n",
    "    #print(flinesum[1])\n",
    "    #abc = flinesum[1]\n",
    "    #print(flinesum[2])\n",
    "    #bcd = flinesum[2]\n",
    "    #print(flinesum[3])\n",
    "    #cde = flinesum[3]\n",
    "\n",
    "    def cleanSentences(string):\n",
    "        \n",
    "        print(\"cleaning\")\n",
    "        string = string.lower().replace(\"<br />\", \" \") \n",
    "        return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "        #string = [[string.lower().replace(\"<br />\", \" \") for word in text.split()] for text in flinesum]\n",
    "        #return [re.sub(strip_special_chars, \"\", string.lower() for word in text.split() for text in flinesum)]\n",
    "\n",
    "        #return re.sub(strip_special_chars, \"\", string.lower())\n",
    "    #cleanedLine = [[flinesum.lower().replace(\"<br />\", \" \") for word in text.split()] for text in flinesum]\n",
    "\n",
    "    cleanedLine = cleanSentences(xyz)\n",
    "    split = cleanedLine.split()\n",
    "    #split = [[cleanedLine.split() for word in text.split()] for text in flinesum]\n",
    "    #print(split)\n",
    "    \n",
    "     \n",
    "    for word in split:\n",
    "        #print(indexCounter)\n",
    "        #print(n_tokens)\n",
    "        #if indexCounter < maxSeqLength:\n",
    "        if indexCounter < n_tokens:\n",
    "            try:\n",
    "                firstFile[indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "            indexCounter = indexCounter + 1\n",
    "firstFile\n",
    "\n",
    "# print(string)\n",
    "    #\n",
    "    #texts = [[word.lower() for word in text.split()] for text in data]\n",
    "    #return [re.sub(strip_special_chars, \"\", string.lower() for word in string]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "##import re\n",
    "##strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "##firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "##with open(fname) as f:\n",
    "  ##  indexCounter = 0\n",
    "    #sumline = Summarize(f.readline)\n",
    "    #line=sumline.readline()\n",
    "    \n",
    "   ## line=f.read()\n",
    "   ## flinesum=Summarize(line)\n",
    "    \n",
    "    ##texts = [[word.lower() for word in text.split()] for text in data]\n",
    "\n",
    "    # Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "\n",
    "#def cleanSentences(string):\n",
    " #   print(\"cleaning\")\n",
    "  #  string = [[string.lower().replace(\"<br />\", \" \") for word in text.split()] for text in flinesum]\n",
    "   # print(string)\n",
    "    #xyz = [re.sub(strip_special_chars, \"\", string.lower()) for word in string]\n",
    "    #return(xyz)\n",
    "    #texts = [[word.lower() for word in text.split()] for text in data]\n",
    "    #return [re.sub(strip_special_chars, \"\", string.lower() for word in string]\n",
    "   # return re.sub(strip_special_chars, \"\", string.lower())\n",
    "# print str(my_list3).strip('[]')\n",
    "    \n",
    "    #for lines in new_file:\n",
    "     #   print(line)\n",
    "    \n",
    "    #line=f.readline()\n",
    "    #print(flinesum)\n",
    "    \n",
    "    #flinesum1 = flinesum.readlines()\n",
    "    #n = 0\n",
    "    #for words in flinesum[n]:\n",
    "\n",
    "    #split = flinesum[n].split()\n",
    "    #n = n+1\n",
    "    \n",
    "    #cleanedLine = cleanSentences(flinesum.\n",
    "    ##print(cleanedLine)\n",
    "    \n",
    "    \n",
    "    #print(flinesum[i]) \n",
    "    ##string1 = string(flinesum)\n",
    "    \n",
    "    \n",
    "    ##cleanedLine = [[flinesum.lower().replace(\"<br />\", \" \") for word in text.split()] for text in flinesum]\n",
    "      # print(cleanedLine)\n",
    "    \n",
    "    #cleanedLine = cleanSentences(flinesum.string())\n",
    "    \n",
    "    #split = flinesum.split() \n",
    "\n",
    "\n",
    "    ##split = cleanedLine.split()\n",
    "    ##for word in split:\n",
    "    ##print(split)\n",
    "    \n",
    "    \n",
    "    #print(flinesum[0])\n",
    "    #for word in flinesum:\n",
    "     #   if indexCounter < 250:\n",
    "      #      try:\n",
    "       #         flinesum[indexCounter] = wordsList.index(word)\n",
    "        #    except ValueError:\n",
    "         #       flinesum[indexCounter] = 399999 #Vector for unknown words\n",
    "       # indexCounter = indexCounter + 1\n",
    "#firstFile\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##for word in split:\n",
    "       ## if indexCounter < maxSeqLength:\n",
    "          ##  try:\n",
    "           ##     firstFile[indexCounter] = wordsList.index(word)\n",
    "           ## except ValueError:\n",
    "           ##     firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        ##indexCounter = indexCounter + 1\n",
    "##firstFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's do the same for each of our 25,000 reviews. We'll load in the movie training set and integerize it to get a 25000 x 250 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    " ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    " fileCounter = 0\n",
    " for pf in positiveFiles:\n",
    "    with open(pf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            try:\n",
    "                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "        fileCounter = fileCounter + 1 \n",
    "\n",
    " for nf in negativeFiles:\n",
    "    with open(nf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            try:\n",
    "                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "        fileCounter = fileCounter + 1 \n",
    "# #Pass into embedding function and see if it evaluates. \n",
    "\n",
    " np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.load('idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below you can find a couple of helper functions that will be useful when training the network in a later step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we’re ready to start creating our Tensorflow graph. We’ll first need to define some hyperparameters, such as batch size, number of LSTM units, number of output classes, and number of training iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As with most Tensorflow graphs, we’ll now need to specify two placeholders, one for the inputs into the network, and one for the labels. The most important part about defining these placeholders is understanding each of their dimensionalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The labels placeholder represents a set of values, each either [1, 0] or [0, 1], depending on whether each training example is positive or negative. Each row in the integerized input placeholder represents the integerized representation of each training example that we include in our batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once we have our input data placeholder, we’re going to call the tf.nn.lookup() function in order to get our word vectors. The call to that function will return a 3-D Tensor of dimensionality batch size by max sequence length by word vector dimensions. In order to visualize this 3-D tensor, you can simply think of each data point in the integerized input tensor as the corresponding D dimensional vector that it refers to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "#![caption](Images/SentimentAnalysis13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have the data in the format that we want, let’s look at how we can feed this input into an LSTM network. We’re going to call the tf.nn.rnn_cell.BasicLSTMCell function. This function takes in an integer for the number of LSTM units that we want. This is one of the hyperparameters that will take some tuning to figure out the optimal value. We’ll then wrap that LSTM cell in a dropout layer to help prevent the network from overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we’ll feed both the LSTM cell and the 3-D tensor full of input data into a function called tf.nn.dynamic_rnn. This function is in charge of unrolling the whole network and creating a pathway for the data to flow through the RNN graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)# Initialize the parameter of RNN cell\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As a side note, another more advanced network architecture choice is to stack multiple LSTM cells on top of each other. This is where the final hidden state vector of the first LSTM feeds into the second. Stacking these cells is a great way to help the model retain more long term dependence information, but also introduces more parameters into the model, thus possibly increasing the training time, the need for additional training examples, and the chance of overfitting. For more information on how you can add stacked LSTMs to your model, check out Tensorflow's excellent [documentation](https://www.tensorflow.org/tutorials/recurrent#stacking_multiple_lstms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first output of the dynamic RNN function can be thought of as the last hidden state vector. This vector will be reshaped and then multiplied by a final weight matrix and a bias term to obtain the final output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we’ll define correct prediction and accuracy metrics to track how the network is doing. \n",
    "#The correct prediction formulation works by looking at the index of the maximum value of the 2 output values, and then seeing whether it matches with the training labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’ll define a standard cross entropy loss with a softmax layer put on top of the final prediction values. \n",
    "#For the optimizer, we’ll use Adam and the default learning rate of .001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you’d like to use Tensorboard to visualize the loss and accuracy values, you can also run and the modify the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The basic idea of the training loop is that we first define a Tensorflow session. Then, we load in a batch of reviews and their associated labels. Next, we call the session’s `run` function. This function has two arguments. The first is called the \"fetches\" argument. It defines the value we’re interested in computing. We want our optimizer to be computed since that is the component that minimizes our loss function. The second argument is where we input our `feed_dict`. This data structure is where we provide inputs to all of our placeholders. We need to feed our batch of reviews and our batch of labels. This loop is then repeated for a set number of training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of training the network in this notebook (which will take at least a couple of hours), we’ll load in a pretrained model.\n",
    "\n",
    "#If you decide to train this notebook on your own machine, note that you can track its progress using [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard). While the following cell is running, use your terminal to enter the directory that contains this notebook, enter `tensorboard --logdir=tensorboard`, and visit http://localhost:6006/ with a browser to keep an eye on your training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading on my system \n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "    #Write summary to Tensorboard\n",
    "   if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    #Save the network every 10,000 training iterations\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy and loss curves during training can be found below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis6.png)\n",
    "![caption](Images/SentimentAnalysis7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the training curves above, it seems that the model's training is going well. The loss is decreasing steadily, and the accuracy is approaching 100 percent. However, when analyzing training curves, we should also pay special attention to the possibility of our model overfitting the training dataset. Overfitting is a common phenomenon in machine learning where a model becomes so fit to the training data that it loses the ability to generalize to the test set. This means that training a network until you achieve 0 training loss might not be the best way to get an accurate model that performs well on data it has never seen before. Early stopping is an intuitive technique commonly used with LSTM networks to combat this issue. The basic idea is that we train the model on our training set, while also measuring its performance on the test set every now and again. Once the test error stops its steady decrease and begins to increase instead, you'll know to stop training, since this is a sign that the network has begun to overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a pretrained model involves defining another Tensorflow session, creating a Saver object, and then using that object to call the restore function. This function takes into 2 arguments, one for the current session, and one for the name of the saved model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm-90000.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we’ll load some movie reviews from our test set. Remember, these are reviews that the model has not been trained on and has never seen before. The accuracy for each test batch can be seen when you run the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#start_time = time.time()\n",
    "#main()\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy for this batch:', 91.66666865348816)\n",
      "('Accuracy for this batch:', 83.33333134651184)\n",
      "('Accuracy for this batch:', 91.66666865348816)\n",
      "('Accuracy for this batch:', 95.83333134651184)\n",
      "('Accuracy for this batch:', 83.33333134651184)\n",
      "('Accuracy for this batch:', 79.16666865348816)\n",
      "('Accuracy for this batch:', 83.33333134651184)\n",
      "('Accuracy for this batch:', 83.33333134651184)\n",
      "('Accuracy for this batch:', 79.16666865348816)\n",
      "('Accuracy for this batch:', 75.0)\n",
      "1.398654\n"
     ]
    }
   ],
   "source": [
    "##import cProfile\n",
    "##cProfile.run('foo()')\n",
    "#python -m cProfile myscript.py\n",
    "#python -m cProfile %1\n",
    "#profile euler048.py\n",
    "\n",
    "##import timeit\n",
    "\n",
    "##def foo():\n",
    "##    print 'bar'\n",
    "\n",
    "##def dotime():\n",
    "##    t = timeit.Timer(\"foo()\",globals=globals())\n",
    "##    time = t.timeit(1)\n",
    "##    print \"took %fs\\n\" % (time,)\n",
    "\n",
    "##import __builtin__\n",
    "##__builtin__.__dict__.update(locals())\n",
    "\n",
    "##dotime()\n",
    "\n",
    "#import timeit\n",
    "\n",
    "#def foo():\n",
    "\n",
    "##import time\n",
    "##start_time = time.time()\n",
    "##main()\n",
    "##print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "import time\n",
    "start = time.clock()\n",
    "  \n",
    "\n",
    "#def main():\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    #xyz1 = main()\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "    \n",
    "print time.clock() - start\n",
    "\n",
    "\n",
    "    \n",
    "#def dotime():\n",
    "#   t = timeit.Timer(\"foo()\",\"from_main_import foo\")\n",
    "#    time = t.timeit(1)\n",
    "#    print \"took %fs\\n\" % (time,)\n",
    "\n",
    "#dotime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
